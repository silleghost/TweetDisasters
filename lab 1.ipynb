{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of tweets labeled as either disaster-related (1) or non-disaster (0). Initial exploration shows:\n",
    "- Columns: `id`, `keyword`, `location`, `text`, `target`.\n",
    "- `keyword` and `location` have missing values, so the focus will be on the `text` column for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose NLTK’s TweetTokenizer for tokenization due to its specialized handling of Twitter data. Unlike generic tokenizers, it effectively preserves **hashtags** (e.g., #NLP), @mentions (e.g., @username), emojis, and contractions (e.g., \"don’t\" - \"do\", \"n’t\"), ensuring Twitter-specific elements remain intact for downstream analysis.\n",
    "\n",
    "For lemmatization, WordNetLemmatizer was selected because it leverages WordNet’s lexical database and considers word context (via part-of-speech tagging). This allows it to produce accurate base forms (e.g., converting \"running\" to \"run\" when tagged as a verb), making it ideal for nuanced text processing in English.\n",
    "\n",
    "To compare lemmatization with a faster but less context-aware approach, PorterStemmer was included. While stemming aggressively truncates suffixes (e.g., \"running\" → \"run\", \"happily\" → \"happili\"), it offers computational efficiency, which is advantageous for large datasets.\n",
    "\n",
    "Moreover, I decided to try using stopwords list, which filters some extra words, thus reducing overall dimensionality and improving text proccessing. Tweets' hashtags are treated as full words, excluding '#' symbol. Hashtags like `#wildfires` are split into constituent words (\"wildfires\") to convert them into meaningful tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nixos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nixos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "download('wordnet')\n",
    "download('stopwords')\n",
    "\n",
    "tk = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) - set(['not', 'no']) #prevent certain words from deleting as they may contain essential clarification (e.g., fire != no fire)\n",
    "\n",
    "#Applying tokenization\n",
    "train_df['tokenized'] = train_df['text'].apply(lambda tweet: tk.tokenize(tweet))\n",
    "\n",
    "#Replace hashtags with words\n",
    "def remove_hashtags(token):\n",
    "    hashtags = re.findall(r'#\\w+', token)\n",
    "    for hashtag in hashtags:\n",
    "        words = re.findall(r'[A-Z]?[a-z]+', hashtag[1:])\n",
    "        token = token.replace(hashtag, ' '.join(words))\n",
    "    \n",
    "    return token\n",
    "\n",
    "#Remove words from stoplist\n",
    "def remove_stopwords(text):\n",
    "    filtered_tokens = [word for word in text if word.lower() not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "#Preproccessing\n",
    "train_df['preprocessed'] = train_df['tokenized'].apply(lambda list: remove_stopwords([remove_hashtags(token) for token in list]))\n",
    "\n",
    "\n",
    "#Lemmatization and stemming\n",
    "train_df['lemmatized'] = train_df['preprocessed'].apply(lambda list: ' '.join([lemmatizer.lemmatize(token) for token in list]))\n",
    "train_df['stemmed'] = train_df['preprocessed'].apply(lambda list: ' '.join([stemmer.stem(token) for token in list]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
       "      <td>[Deeds, Reason, earthquake, May, ALLAH, Forgiv...</td>\n",
       "      <td>Deeds Reason earthquake May ALLAH Forgive u</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>[Forest, fire, near, La, Ronge, Sask, ., Canada]</td>\n",
       "      <td>Forest fire near La Ronge Sask . Canada</td>\n",
       "      <td>forest fire near la rong sask . canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[All, residents, asked, to, ', shelter, in, pl...</td>\n",
       "      <td>[residents, asked, ', shelter, place, ', notif...</td>\n",
       "      <td>resident asked ' shelter place ' notified offi...</td>\n",
       "      <td>resid ask ' shelter place ' notifi offic . no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>[13,000, people, receive, wildfires, evacuatio...</td>\n",
       "      <td>13,000 people receive wildfire evacuation orde...</td>\n",
       "      <td>13,000 peopl receiv wildfir evacu order califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
       "      <td>[got, sent, photo, Ruby, Alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo Ruby Alaska smoke wildfire pour...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                          tokenized  \\\n",
       "0       1  [Our, Deeds, are, the, Reason, of, this, #eart...   \n",
       "1       1   [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2       1  [All, residents, asked, to, ', shelter, in, pl...   \n",
       "3       1  [13,000, people, receive, #wildfires, evacuati...   \n",
       "4       1  [Just, got, sent, this, photo, from, Ruby, #Al...   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  [Deeds, Reason, earthquake, May, ALLAH, Forgiv...   \n",
       "1   [Forest, fire, near, La, Ronge, Sask, ., Canada]   \n",
       "2  [residents, asked, ', shelter, place, ', notif...   \n",
       "3  [13,000, people, receive, wildfires, evacuatio...   \n",
       "4  [got, sent, photo, Ruby, Alaska, smoke, wildfi...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0        Deeds Reason earthquake May ALLAH Forgive u   \n",
       "1            Forest fire near La Ronge Sask . Canada   \n",
       "2  resident asked ' shelter place ' notified offi...   \n",
       "3  13,000 people receive wildfire evacuation orde...   \n",
       "4  got sent photo Ruby Alaska smoke wildfire pour...   \n",
       "\n",
       "                                             stemmed  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1             forest fire near la rong sask . canada  \n",
       "2  resid ask ' shelter place ' notifi offic . no ...  \n",
       "3  13,000 peopl receiv wildfir evacu order califo...  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Models\n",
    "For basic ML approach I have chosen three different models: \n",
    "- LogisticRegression, which works well with high-dimensional text data and provides interpretability via coefficients.\n",
    "   - **Parameters**: Grid search over `C` (regularization), `penalty` (L1/L2), and `solver`.\n",
    "\n",
    "- Support Vector Machine (SVM), which is effective in high-dimensional spaces with kernel tricks (tested: linear, RBF kernels).\n",
    "   - **Parameters**: Optimized `C`, `kernel`, and `gamma`.\n",
    "\n",
    "- Naive Bayes (NB), which is fast and suitable for sparse text data with Laplace smoothing (`alpha`) and also known to work well in text classification.\n",
    "   - **Parameters**: Tested `alpha` (smoothing) and `fit_prior` (class balance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting in train/test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = train_df.pop('target')\n",
    "X_train_lem, X_val_lem, y_train, y_val = train_test_split(train_df['lemmatized'], y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_train_stem, X_val_stem, y_train, y_val = train_test_split(train_df['stemmed'], y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training pipeline includes different vectorizers such as CountVectorizer and TfidfVectorizer.\n",
    ">CountVectorizer is a pre-processing technique used to convert text data into numerical form. This creates a bag of words where each word is treated as a separate feature and the count of each word in a given document is used as the value of that feature.\n",
    "\n",
    ">TfidfVectorizer is based on the logic that words that are too abundant in a corpus and words that are too rare are both not statistically important for finding a pattern. The Logarithmic factor in tfidf mathematically penalizes the words that are too abundant or too rare in the corpus by giving them low tfidf scores.\n",
    "\n",
    "Both of them are used for vectorizing texts and support context, so we will try them in comparance to find the best option.\n",
    "\n",
    "The full pipeline uncludes testing of the following combinations:\n",
    "- LogisticRegression with CountVectorizer and lemmatized text\n",
    "- SVM with CountVectorizer and lemmatized text\n",
    "- Multinomial Naive Bayes with CountVectorizer and lemmatized text\n",
    "- LogisticRegression with TfidfVectorizer and stemmed text\n",
    "- SVM with TfidfVectorizer and stemmed text\n",
    "- Multinomial Naive Bayes with TfidfVectorizer and stemmed text\n",
    "- Multinomial Naive Bayes with CountVectorizer and stemmed text\n",
    "\n",
    "For every option GridSearch is used for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training bow_LogReg ===\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/nix/store/fm78bn090czdx0zna8vxdzg7mghdxip7-python3-3.12.7-env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training bow_SVM ===\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "=== Training bow_MultiNB ===\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "\n",
      "=== Training stem_LogReg ===\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "\n",
      "=== Training stem_SVM ===\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "=== Training stem_MultiNB ===\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "=== Training bow_MultiNB_stem ===\n",
      "Fitting 3 folds for each of 14 candidates, totalling 42 fits\n",
      "\n",
      "Best model: bow_MultiNB_stem\n",
      "Best F1-score: 0.7437\n",
      "Best params: {'model__alpha': 2.0, 'model__fit_prior': True, 'vectorizer__max_features': 20000, 'vectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pipelines with parameter grids\n",
    "param_grids = {\n",
    "    'bow_LogReg': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('model', LogisticRegression(max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "            'vectorizer__max_features': [5000, 10000],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__penalty': ['l1', 'l2'],\n",
    "            'model__solver': ['liblinear', 'saga']\n",
    "        },\n",
    "        'data': 'lem'  # using lemmatized data\n",
    "    },\n",
    "    \n",
    "    'bow_SVM': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('model', SVC())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,2)],\n",
    "            'vectorizer__max_features': [10000],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__kernel': ['linear', 'rbf'],\n",
    "            'model__gamma': ['scale', 'auto']\n",
    "        },\n",
    "        'data': 'lem' # using lemmatized data\n",
    "    },\n",
    "    \n",
    "    'bow_MultiNB': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('model', MultinomialNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,1)],\n",
    "            'vectorizer__max_features': [20000],\n",
    "            'model__alpha': [0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0],\n",
    "            'model__fit_prior': [True, False]\n",
    "        },\n",
    "        'data': 'lem' # using lemmatized data\n",
    "    },\n",
    "    \n",
    "    'stem_LogReg': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', LogisticRegression(max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "            'vectorizer__max_features': [5000, 10000],\n",
    "            'vectorizer__use_idf': [True, False],\n",
    "            'model__C': [0.1, 1, 10],\n",
    "            'model__penalty': ['l2'],\n",
    "            'model__solver': ['sag', 'saga']\n",
    "        },\n",
    "        'data': 'stem'  # using stem data\n",
    "    },\n",
    "    \n",
    "    'stem_SVM': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', SVC())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,2)],\n",
    "            'vectorizer__max_features': [10000],\n",
    "            'model__C': [1, 10],\n",
    "            'model__kernel': ['linear'],\n",
    "            'model__gamma': ['scale']\n",
    "        },\n",
    "        'data': 'stem' # using stem data\n",
    "    },\n",
    "    \n",
    "    'stem_MultiNB': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer()),\n",
    "            ('model', MultinomialNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,1)],\n",
    "            'vectorizer__max_features': [20000],\n",
    "            'model__alpha': [0.1, 1.0],\n",
    "            'model__fit_prior': [True]\n",
    "        },\n",
    "        'data': 'stem' # using stem data\n",
    "    },\n",
    "\n",
    "    #Additional option\n",
    "    'bow_MultiNB_stem': {\n",
    "        'pipeline': Pipeline([\n",
    "            ('vectorizer', CountVectorizer()),\n",
    "            ('model', MultinomialNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'vectorizer__ngram_range': [(1,1)],\n",
    "            'vectorizer__max_features': [20000],\n",
    "            'model__alpha': [0.01, 0.1, 0.5, 1.0, 1.5, 2.0, 5.0],\n",
    "            'model__fit_prior': [True, False]\n",
    "        },\n",
    "        'data': 'stem' # using stem data\n",
    "    }\n",
    "}\n",
    "\n",
    "# GridSearch\n",
    "best_models = {}\n",
    "\n",
    "for name, config in param_grids.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    \n",
    "    X_train = X_train_lem if config['data'] == 'lem' else X_train_stem\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=config['pipeline'],\n",
    "        param_grid=config['params'],\n",
    "        scoring='f1',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = {\n",
    "        'model': grid_search.best_estimator_,\n",
    "        'score': grid_search.best_score_,\n",
    "        'params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Best model score\n",
    "best_model_info = max(best_models.items(), key=lambda x: x[1]['score'])\n",
    "print(f\"\\nBest model: {best_model_info[0]}\")\n",
    "print(f\"Best F1-score: {best_model_info[1]['score']:.4f}\")\n",
    "print(f\"Best params: {best_model_info[1]['params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on F1-score, Multinomial Naive Bayes with stemmed text was picked as the best model and will be used to make predictions for the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again apply necessary transformations to make our test data suitable for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying tokenization\n",
    "test_df['tokenized'] = test_df['text'].apply(lambda tweet: tk.tokenize(tweet))\n",
    "#Preproccessing\n",
    "test_df['preprocessed'] = train_df['tokenized'].apply(lambda list: remove_stopwords([remove_hashtags(token) for token in list]))\n",
    "#Lemmatization and stemming\n",
    "test_df['lemmatized'] = test_df['preprocessed'].apply(lambda list: ' '.join([lemmatizer.lemmatize(token) for token in list]))\n",
    "test_df['stemmed'] = test_df['preprocessed'].apply(lambda list: ' '.join([stemmer.stem(token) for token in list]))\n",
    "\n",
    "# Best model prediction\n",
    "if 'stem' in best_model_info[0]:\n",
    "    X_test = test_df['stemmed']\n",
    "else:\n",
    "    X_test = test_df['lemmatized']\n",
    "\n",
    "final_model = best_model_info[1]['model']\n",
    "y_pred = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming + NB performed better than lemmatization, likely due to reduced dimensionality without losing critical signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  target\n",
      "0   0       1\n",
      "1   2       1\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': y_pred\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Choices\n",
    "Tweets contain contextual dependencies where word order matters (e.g., \"flood warning\" vs. \"warning flood\"). LSTM/GRU cells address vanishing gradients in vanilla RNNs, enabling long-term dependency capture. \n",
    "\n",
    "Bidirectional Layers process text in both forward/backward directions to capture context from past *and* future tokens. Critical for phrases like *\"not safe\"* where negation (\"not\") informs the meaning of subsequent words. \n",
    "\n",
    "Embedding Layer converts tokens to dense vectors (`embedding_dim=100-200`) to represent semantic relationships. Larger dimensions (e.g., 200) help capture nuanced meanings but risk overfitting on small datasets.\n",
    "\n",
    "**Hyperparameter Choices**:\n",
    "   - **Hidden Dimension**: `128-256` balances model capacity and computational cost. Larger sizes (256) improve context retention but require more data.\n",
    "   - **Dropout**: `0.3-0.5` regularizes the model by randomly disabling neurons, mitigating overfitting on noisy tweet data.\n",
    "   - **Layers**: Stacked LSTM/GRU layers (`n_layers=2`) learn hierarchical features but increase complexity. Single layers suffice for shorter texts like tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen stemming Over Lemmatization, because it reduces vocabulary size (e.g., \"running\" → \"run\") while retaining disaster-related root words. Critical for computational efficiency in embedding layers.\n",
    "All sequences fixed to `max_len=100`. Shorter tweets padded with `<pad>`; longer ones truncated to avoid noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, max_vocab=20000):\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english')) - {'not', 'no'}\n",
    "        self.vocab = {}\n",
    "        self.max_vocab = max_vocab\n",
    "\n",
    "    def remove_hashtags(self, token):\n",
    "        hashtags = re.findall(r'#\\w+', token)\n",
    "        for hashtag in hashtags:\n",
    "            words = re.findall(r'[A-Z]?[a-z]+', hashtag[1:])\n",
    "            token = token.replace(hashtag, ' '.join(words))\n",
    "        \n",
    "        return token\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        processed = [\n",
    "            self.remove_hashtags(token) \n",
    "            for token in tokens\n",
    "            if token.lower() not in self.stop_words\n",
    "        ]\n",
    "        \n",
    "        return ' '.join([self.stemmer.stem(token) for token in processed])\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        counts = Counter()\n",
    "        for text in processed_texts:\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            counts.update(tokens)\n",
    "        \n",
    "        vocab = ['<pad>', '<unk>'] + [word for word, _ in counts.most_common(self.max_vocab-2)]\n",
    "        self.vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    def text_to_sequence(self, text, max_len=100):\n",
    "        tokens = self.tokenizer.tokenize(text)[:max_len]\n",
    "        sequence = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "        return sequence + [self.vocab['<pad>']] * (max_len - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, processor, max_len=100):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        sequence = self.processor.text_to_sequence(text, self.max_len)\n",
    "        if self.labels is not None:\n",
    "            return torch.LongTensor(sequence), torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return torch.LongTensor(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding Layer converts token IDs to dense vectors.\n",
    "- LSTM/GRU processes sequences step-by-step, updating hidden states to retain context.\n",
    "- Bidirectional layers concatenate forward/backward outputs for richer representations.\n",
    "- Final Hidden State: Used as the tweet's \"summary\" for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TweetClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, n_layers=2, dropout=0.5, bidirectional=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "\n",
    "        if self.lstm.bidirectional:\n",
    "                hidden = self.dropout(torch.cat((hidden[-2], hidden[-1]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, \n",
    "                 n_layers=1, dropout=0.3, rnn_type='lstm'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if rnn_type.lower() == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        elif rnn_type.lower() == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type\")\n",
    "            \n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        if isinstance(hidden, tuple):  # For LSTM\n",
    "            hidden = hidden[0]\n",
    "            \n",
    "        out = self.fc(self.dropout(hidden[-1]))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss Function: Cross-entropy loss compares predicted probabilities vs. true labels (disaster/non-disaster).\n",
    "- Optimizer: Adam (adaptive learning rate) with `lr=0.001` balances speed and stability.\n",
    "- Gradient Clipping: Prevents exploding gradients in RNNs (`max_norm=1.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, train_df, test_df):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.results = []\n",
    "        self.best_model = None\n",
    "        self.best_acc = 0\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def run_experiments(self, configs, epochs=10, batch_size=64):\n",
    "        processor = TextProcessor(max_vocab=20000)\n",
    "        processor.build_vocab(train_df['text'])\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.train_df['text'], self.train_df['target'],\n",
    "            test_size=0.2, stratify=self.train_df['target'], random_state=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = TweetDataset(X_train, y_train, processor)\n",
    "        val_dataset = TweetDataset(X_val, y_val, processor)\n",
    "        \n",
    "        for config in configs:\n",
    "            print(f\"\\nRunning experiment with config: {config}\")\n",
    "            \n",
    "            model = RNNClassifier(\n",
    "                vocab_size=len(processor.vocab),\n",
    "                **config\n",
    "            )\n",
    "            model.to(self.device)\n",
    "            \n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.get('lr', 0.001))\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                val_acc = self.evaluate(model, val_loader, self.device)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | Val Acc: {val_acc:.4f}\")\n",
    "                \n",
    "                if val_acc > self.best_acc:\n",
    "                    self.best_acc = val_acc\n",
    "                    self.best_model_info = {\n",
    "                        'config': config,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'vocab_size': len(processor.vocab)\n",
    "                    }\n",
    "                    \n",
    "            self.results.append({\n",
    "                **config,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "            \n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        print(\"\\nExperiment Results:\")\n",
    "        print(results_df.sort_values('val_acc', ascending=False))\n",
    "        \n",
    "    def evaluate(self, model, data_loader, device):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "        return correct / total\n",
    "    \n",
    "    def predict(self, test_df):\n",
    "        if not self.best_model_info:\n",
    "            raise ValueError(\"Train model first!\")\n",
    "            \n",
    "        processor = TextProcessor()\n",
    "        processor.build_vocab(self.train_df['text'])\n",
    "        \n",
    "        model = RNNClassifier(\n",
    "            vocab_size=self.best_model_info['vocab_size'],\n",
    "            **self.best_model_info['config']\n",
    "        )\n",
    "        \n",
    "        model.to(self.device)\n",
    "        \n",
    "        model.load_state_dict(self.best_model_info['state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        test_dataset = TweetDataset(test_df['text'], None, processor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment with different parameters 3 configs are used with the ability of adding additional ones. They have different amount of layers, different dimensionality. But the main feature to experiment with is the type of the RNN. While LSTM supports long-term memory which is often useful for translation and long text analyzing, GRU is simplified version which, to my mind, will be the best to analyze such short messages as tweets. For classifying tweets (short texts, local context), **GRU** is often sufficient and more effective. However, if tweets contain complex contextual relationships (e.g., sarcasm, multi-step events), **LSTM** may show better accuracy by preserving long-term dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_configs = [\n",
    "    {'embedding_dim': 100, 'hidden_dim': 128, 'n_layers': 1, 'dropout': 0.3, 'rnn_type': 'lstm'},\n",
    "    {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'rnn_type': 'gru'},\n",
    "    {'embedding_dim': 150, 'hidden_dim': 192, 'n_layers': 1, 'dropout': 0.5, 'rnn_type': 'lstm'},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with config: {'embedding_dim': 100, 'hidden_dim': 128, 'n_layers': 1, 'dropout': 0.3, 'rnn_type': 'lstm'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/7sm98xp06sn7rika9w46239j1d89ckvw-python3.12-torch-2.5.1/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Val Acc: 0.5706\n",
      "Epoch 2/10 | Val Acc: 0.5706\n",
      "Epoch 3/10 | Val Acc: 0.5706\n",
      "Epoch 4/10 | Val Acc: 0.5706\n",
      "Epoch 5/10 | Val Acc: 0.5706\n",
      "Epoch 6/10 | Val Acc: 0.5706\n",
      "Epoch 7/10 | Val Acc: 0.5706\n",
      "Epoch 8/10 | Val Acc: 0.5706\n",
      "Epoch 9/10 | Val Acc: 0.5706\n",
      "Epoch 10/10 | Val Acc: 0.5706\n",
      "\n",
      "Running experiment with config: {'embedding_dim': 200, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.4, 'rnn_type': 'gru'}\n",
      "Epoch 1/10 | Val Acc: 0.4294\n",
      "Epoch 2/10 | Val Acc: 0.5706\n",
      "Epoch 3/10 | Val Acc: 0.6225\n",
      "Epoch 4/10 | Val Acc: 0.6770\n",
      "Epoch 5/10 | Val Acc: 0.6907\n",
      "Epoch 6/10 | Val Acc: 0.7104\n",
      "Epoch 7/10 | Val Acc: 0.7058\n",
      "Epoch 8/10 | Val Acc: 0.7183\n",
      "Epoch 9/10 | Val Acc: 0.7282\n",
      "Epoch 10/10 | Val Acc: 0.7255\n",
      "\n",
      "Running experiment with config: {'embedding_dim': 150, 'hidden_dim': 192, 'n_layers': 1, 'dropout': 0.5, 'rnn_type': 'lstm'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/7sm98xp06sn7rika9w46239j1d89ckvw-python3.12-torch-2.5.1/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Val Acc: 0.5706\n",
      "Epoch 2/10 | Val Acc: 0.5706\n",
      "Epoch 3/10 | Val Acc: 0.5706\n",
      "Epoch 4/10 | Val Acc: 0.5706\n",
      "Epoch 5/10 | Val Acc: 0.5706\n",
      "Epoch 6/10 | Val Acc: 0.5706\n",
      "Epoch 7/10 | Val Acc: 0.5706\n",
      "Epoch 8/10 | Val Acc: 0.5706\n",
      "Epoch 9/10 | Val Acc: 0.5706\n",
      "Epoch 10/10 | Val Acc: 0.5706\n",
      "\n",
      "Experiment Results:\n",
      "   embedding_dim  hidden_dim  n_layers  dropout rnn_type   val_acc\n",
      "1            200         256         2      0.4      gru  0.725542\n",
      "0            100         128         1      0.3     lstm  0.570584\n",
      "2            150         192         1      0.5     lstm  0.570584\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "runner = ExperimentRunner(train_df, test_df)\n",
    "runner.run_experiments(experiment_configs, epochs=10)\n",
    "\n",
    "predictions = runner.predict(test_df)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission_df.to_csv('best_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is a **pre-trained language model** developed by Google in 2018. It is based on the **transformer** architecture and has revolutionised NLP by learning to understand the context of words in two directions (left to right and right to left).\n",
    "\n",
    "For tweet classification, BERT excels because tweets often rely on subtle context, sarcasm, or localized slang. For instance, the phrase \"fire on the mountain\" could indicate disaster or metaphor. BERT’s bidirectional attention captures such nuances by analyzing how words interact across the entire tweet. Additionally, its pretrained knowledge of language patterns allows it to generalize well even with limited labeled disaster data. Fine-tuning BERT on task-specific data adapts these universal language features to identify keywords (e.g., \"evacuation,\" \"flood\") while filtering noise like informal abbreviations or hashtags. This makes it superior to simpler models for short, context-dependent text.ё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetBERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx] if self.labels is not None else -1\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long) if label != -1 else torch.tensor(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_df.text.values,\n",
    "    train_df.target.values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df.target.values\n",
    ")\n",
    "\n",
    "def create_data_loader(texts, labels, tokenizer, max_len=128, batch_size=16):\n",
    "    dataset = TweetBERTDataset(\n",
    "        texts=texts,\n",
    "        labels=labels,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nix/store/cbb024c5q6lds8bvib6j064ff1d8vx0z-python3.12-transformers-4.47.0/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 128\n",
    "\n",
    "train_loader = create_data_loader(train_texts, train_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(val_texts, val_labels, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_loader = create_data_loader(test_df.text.values, None, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "EPOCHS = 3\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "Train loss: 0.46010255981774467\n",
      "Validation accuracy: 0.8220617202889035\n",
      "Epoch 2/3\n",
      "----------\n",
      "Train loss: 0.30855737548820145\n",
      "Validation accuracy: 0.8371634931057124\n",
      "Epoch 3/3\n",
      "----------\n",
      "Train loss: 0.22307961451970137\n",
      "Validation accuracy: 0.8207485226526592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/nix-shell-102548-0/nix-shell-118710-0/ipykernel_134743/1956144969.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_bert_model.bin'))\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    \n",
    "    val_acc = eval_model(model, val_loader, device)\n",
    "    print(f'Validation accuracy: {val_acc}')\n",
    "    \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_bert_model.bin')\n",
    "        best_accuracy = val_acc\n",
    "\n",
    "model.load_state_dict(torch.load('best_bert_model.bin'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': predictions\n",
    "})\n",
    "submission_df.to_csv('bert_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experimental results highlight a clear progression in performance across the three approaches, directly correlated with model complexity and contextual understanding.\n",
    "\n",
    "1. The classical ML models (Logistic Regression, SVM, Naive Bayes) achieved a *Kaggle* score of **0.54183**, reflecting their limitations in handling noisy, context-dependent text like tweets. While techniques such as stemming and stopword removal improved feature quality, these models struggled with semantic nuances—for example, distinguishing metaphorical phrases (\"fire in the sky\" vs. literal disasters) or sarcasm. Their reliance on bag-of-words representations inherently ignores word order and context, leading to suboptimal performance despite computational efficiency.\n",
    "\n",
    "2. The neural network approach (RNN/GRU) improved the score to **0.76739**, demonstrating the advantage of sequence-aware architectures. By processing text bidirectionally and capturing local dependencies, these models better interpreted phrases like \"no fire\" or \"evacuation ordered.\" However, their performance plateaued due to limited pretrained knowledge and an inability to grasp deeper semantic relationships (e.g., linking \"flood\" with \"rescue\" across a tweet). Training time increased moderately, but the trade-off was justified by the accuracy gain.\n",
    "\n",
    "3. BERT achieved the highest score (**0.82960**), showcasing the power of transformer-based architectures. Its bidirectional attention mechanism resolved ambiguities by analyzing entire tweets holistically—for instance, recognizing that \"storm\" in \"storm of protests\" is metaphorical, while \"storm surge\" indicates a disaster. Pretraining on vast corpora allowed it to generalize patterns (e.g., associating \"power outage\" with disaster reports) even with limited labeled data. However, this came at a computational cost: training BERT required significantly more time and resources, making it less practical for real-time applications without hardware acceleration.\n",
    "\n",
    "### Critical Analysis\n",
    "The notebook’s preprocessing pipeline—particularly hashtag decomposition and negation-aware stopword removal—likely enhanced all models, but BERT benefited most. Classical models lacked the capacity to leverage these refined features fully, while BERT’s self-attention dynamically prioritized relevant tokens (e.g., \"wildfire\" > \"photo\"). Notably, the neural network’s intermediate performance suggests hybrid approaches (e.g., BERT + GRU) could further balance speed and accuracy.\n",
    "\n",
    "To optimize efficiency, I may consider distilled BERT variants (e.g., DistilBERT) or quantization. For classical models, integrating contextual embeddings (e.g., BERT-as-a-feature) might bridge the accuracy gap. Finally, error analysis on misclassified tweets (e.g., sarcastic or ambiguous posts) could guide targeted improvements, such as data augmentation or domain-specific pretraining."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
